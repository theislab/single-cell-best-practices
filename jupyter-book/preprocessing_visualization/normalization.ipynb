{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eceea5f9-1817-43b8-9e2f-3eb2bf8a1688",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4383b510",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8bd4b6",
   "metadata": {},
   "source": [
    "Up to this point, we removed low-quality cells, ambient RNA contamination and doublets from the dataset and the data is available as a count matrix in the form of a numeric matrix of shape cells x genes. These counts represent the capture, reverse transcription and sequencing of a molecule in the scRNA-seq experiment. Each of these steps adds a degree of variability to the measured count depth for identical cells, so the difference in gene expression between cells in the count data might simply be due to sampling effects. This means that the dataset and therefore the count matrix still contains widely varying variance terms. Analyzing the dataset is often challenging as many statistical methods assume data with uniform variance structure. \n",
    "\n",
    "```{admonition} Gamma-Poisson distribution\n",
    "A theoretically and empirically established model for UMI data is the Gamma-Poisson distribution which implies a quadratic mean-variance relation with $Var[Y] = \\mu + \\alpha \\mu^2$ with mean $\\mu$ and overdispersion $\\alpha$. For $\\alpha=0$ this is the Pisson distribution and $\\alpha$ describes the additional variance on top of the Poisson. \n",
    "```\n",
    "\n",
    "The preprocessing step of \"normalization\" aims to adjust the raw counts in the dataset for variable sampling effects by scaling the observable variance to a specified range. Several normalization techniques are used in practice varying in complexity. They are mostly designed in such a whay that subsequent analysis tasks and their underlying statistical methods are applicable. \n",
    "\n",
    "A recent benchmark published by Ahlmann-Eltze and Huber{cite}`Ahlmann-Eltze2021.06.24.449781` compared 22 different transformations for single-cell data. The benchmark compared the performance of the different normalization techniques based on the cell graph overlap with the ground truth. We would like to highlight that a complete benchmark which also compares the impact of the normalization on a variety of different downstream analysis tasks is still outstanding. We advice analysts to chose the normalization carefully and always dependent on the subsequent analysis task. \n",
    "\n",
    "This chapter will introduce the reader to three different normalization techniques, the shifted logarithm transformation, scran normalization and analytic approximation of Pearson residuals. The shifted logarithm works beneficial for stabilizing variance for subsequent dimensionality reduction and identification of differentially expressed genes. Scran was extensively tested and used for batch correction tasks and analytic Pearson residuals are well suited for selecting biologically variable genes and identification of rare cell types. \n",
    "\n",
    "We first import all required Python packages and load the dataset for which we filtered low quality cells, removed ambient RNA and scored doublets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9afa9c69-1dc8-4144-b2aa-48c093933185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import anndata2ri\n",
    "import logging\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "import rpy2.rinterface_lib.callbacks as rcb\n",
    "import rpy2.robjects as ro\n",
    "\n",
    "sc.settings.verbosity = 0\n",
    "sc.settings.set_figure_params(\n",
    "    dpi=80,\n",
    "    facecolor=\"white\",\n",
    "    # color_map=\"YlGnBu\",\n",
    "    frameon=False,\n",
    ")\n",
    "\n",
    "rcb.logger.setLevel(logging.ERROR)\n",
    "ro.pandas2ri.activate()\n",
    "anndata2ri.activate()\n",
    "\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da52c05f-77a4-48f8-a45a-e8de6ab0756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to figshare afterwards\n",
    "adata = sc.read(\"s4d8_subset_gex_qc.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f120db0f-8ff5-46db-bfd7-1dc49c4fc870",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Shifted logarithm \n",
    "\n",
    "The first normalization technique we will introduce is the shifted logarithm which is based in the delta method {ToDo cite}`xx`. The delta method applies a non-linear function $f(Y)$ to the raw counts $Y$ and aims to make the variances across the dataset more similar. The shifted logarithm tackles this by $$f(y) = \\log(\\frac{y}{s}+y_0)$$ with $y$ being the raw counts, $s$ being a so-called size factor and $y_0$ describing a pseudo-count. The size factors are determined for each cell to account for variations in sampling effects and different cell sizes. The size factor for a cell $c$ can be calculated with $$s_c = \\frac{\\sum_c y_{gc}}{L}$$ for a gene $g$ and $L$ describing a target sum. There are different approaches to determine the size factors from the data. We will leverage the scanpy default in this section with $L$ being the median raw count depth in the dataset. Many analysis templates use a fixed values for $L$, for example $L=10^5$, or $L=10^6$ resulting in values commonly known as counts per million (CPM). For a beginner, this values may seem arbitrary, but it can lead to much larger overdispersions than typically seen in single-cell datasets. \n",
    "\n",
    "```{admonition} Overdispersion\n",
    "Overdispersion describes the presence of a greater variability in the dataset than one would expect.\n",
    "```\n",
    "\n",
    "The shifted logarithm is a fast normalization technique, outperforms other methods for uncovering the latent structure of the dataset (especially when followed by principal component analysis) and works beneficial for stabilizing variance for subsequent dimensionality reduction and identification of differentially expressed genes. We will now inspect how to apply this normalization method to our dataset. The shifted logarithm can be convinently called with scanpy by running `pp.normalized_total` with `target_sum=None`. We are setting the `inplace` parameter to `False` as we want to explore three different normalization techniques in this tutorial. The second step now uses the scaled counts and we obtained the first normalized count matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8464a9a9-a31c-40e3-87b6-2da53637f8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales_counts = sc.pp.normalize_total(adata, target_sum=None, inplace=False)\n",
    "# log1p transform\n",
    "adata.layers[\"log1p_norm\"] = sc.pp.log1p(scales_counts[\"X\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b0b9a-7ee3-4979-a8e3-61de9be2e5c4",
   "metadata": {},
   "source": [
    "A second normalization method, which is also based on the delta method, is Scran's pooling-based size factor estimation method. Scran follows the same principles as the shifted logarithm by calculating $$f(y) = \\log(\\frac{y}{s}+y_0)$$ with $y$ being the raw counts, $s$ the size factor and $y_0$ describing a pseudo-count. The only difference now is that Scran leverages a deconvolution approach to estimate the size factors based on a linear regression over genes for pools of cells. This approach aims to better account for differences in count depths across all cells present in the dataset.\n",
    "\n",
    "Cells are partitioned into pools and Scran estimates pool-based size factors using a linear regression over genes. Scran was extensively tested for batch correction tasks and can be easily called with the respective R package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "732cada7-7346-4b03-a74f-34174a7e2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, issparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b90bcbd-0588-4709-a31f-b4d7f01848be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(scran)\n",
    "library(BiocParallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947bae04-7447-413f-aac4-2d4343e43f3e",
   "metadata": {},
   "source": [
    "scran requires a coarse clustering input to improve size factor esimation performance. In this tutorial, we use a simple preprocessing approach and cluster the data at a low resolution to get an input for the size factor estimation. The basic preprocessing includes assuming all size factors are equal (library size normalization to counts per million - CPM) and log-transforming the count data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "129ca6f4-de7a-4f2a-8052-ad7b0ea61049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna.schaar/opt/miniconda3/envs/preprocessing/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Preliminary clustering for differentiated normalisation\n",
    "adata_pp = adata.copy()\n",
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata_pp)\n",
    "sc.pp.pca(adata_pp, n_comps=15)\n",
    "sc.pp.neighbors(adata_pp)\n",
    "sc.tl.leiden(adata_pp, key_added=\"groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b12f01-215c-4b3a-95da-3640dac9e619",
   "metadata": {},
   "source": [
    "We now add `data_mat` and our computed groups into our R environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f89b5d3-356b-4875-91de-dfd0e09a3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "data_mat = adata_pp.X.T\n",
    "# convert to CSC if possible. See https://github.com/MarioniLab/scran/issues/70\n",
    "if scipy.sparse.issparse(data_mat):\n",
    "    if data_mat.nnz > 2**31 - 1:\n",
    "        data_mat = data_mat.tocoo()\n",
    "    else:\n",
    "        data_mat = data_mat.tocsc()\n",
    "ro.globalenv[\"data_mat\"] = data_mat\n",
    "ro.globalenv[\"input_groups\"] = adata_pp.obs[\"groups\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b2316-c2ac-4ce9-b467-1ae3ddb2882a",
   "metadata": {},
   "source": [
    "We can now also delete the copy of our anndata object, as we obtained all objects needed in order to run scran. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bcb4cf1-aede-49d0-bb5c-66d9a80da0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del adata_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862c92ab-828b-4ad5-9ba1-2bd0cc58140f",
   "metadata": {},
   "source": [
    "We now compute the size factors based on the groups of cells we calculated before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91105ba5-9f28-43fc-95cf-9ce054df559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o size_factors\n",
    "\n",
    "size_factors = sizeFactors(\n",
    "    computeSumFactors(\n",
    "        SingleCellExperiment(\n",
    "            list(counts=data_mat)), \n",
    "            clusters = input_groups,\n",
    "            min.mean = 0.1,\n",
    "            BPPARAM = MulticoreParam()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefcc19-4a46-46e9-8f25-78485c47b3f3",
   "metadata": {},
   "source": [
    "We save `size_factors` in `.obs` and are now able to normalize the data and subsequently apply a log1p transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41375b0f-be5f-474e-85ba-c39b66e925aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"size_factors\"] = size_factors\n",
    "scran = adata.X / adata.obs[\"size_factors\"].values[:, None]\n",
    "adata.layers[\"scran_normalization\"] = csr_matrix(sc.pp.log1p(scran))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e5620-0af9-4e46-a7b0-e3932e8d47f0",
   "metadata": {},
   "source": [
    "## Analytic Pearson residuals\n",
    "\n",
    "The third normalization technique we are introducing in this chapter is the analytic approximation of Pearson residuals. This normalization technique was motivated by the observation that cell-to-cell variation in scRNA-seq data might be confounded by biological heterogeneity with technical effects. The method utilizes Pearson residuals from 'regularized negative binomial regression' to calculate a model of technical noise in the data. It explicit adds the count depth as a covariate in a generalized linear model. {cite}`norm:germain_pipecomp_2020` showed in an independent comparison of different normalization techniques that this method removed the impact of sampling effects while preserving cell heterogeneity in the dataset. Notably, analytic Pearson residuals do not require downstream heuristic steps like pseudo count addition or log-transformation.\n",
    "\n",
    "The output of this method are normalized values that can be positive or negative. Negative residuals for a cell and gene indicate that less counts are observed than expected compared to the gene's average expression and cellular sequencing depth. Positive residuals indicate the more counts respectively. Analytic Pearon residuals are implemented in scanpy and can directly be calculated on the raw count matirx. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da5d8bae-6964-499f-bb79-40d759475c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna.schaar/opt/miniconda3/envs/preprocessing/lib/python3.9/site-packages/scanpy/preprocessing/_simple.py:352: RuntimeWarning: invalid value encountered in log1p\n",
      "  np.log1p(X, out=X)\n"
     ]
    }
   ],
   "source": [
    "analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False)\n",
    "adata.layers[\"analytic_pearson_residuals\"] = sc.pp.log1p(analytic_pearson[\"X\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fc1569-27d0-453c-aaf9-08847a87a10d",
   "metadata": {},
   "source": [
    "We applied different normalization techniques to our dataset and saved them as separate layers to our anndata object. Depending on the downstream analysis task it can be favourable to use a differently normalized layer and assess the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e33d217e-6c5c-45ac-9ed9-e1a73ccb5dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write(\"s4d8_subset_gex_qc_norm.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d7c0d3",
   "metadata": {},
   "source": [
    "## Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28855f58-cda0-4280-b8e9-23bfbfe4be72",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    ":labelprefix: norm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee039a",
   "metadata": {},
   "source": [
    "## Contributors\n",
    "\n",
    "We gratefully acknowledge the contributions of:\n",
    "\n",
    "### Authors\n",
    "\n",
    "* Anna Schaar\n",
    "\n",
    "### Reviewers\n",
    "\n",
    "* Lukas Heumos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
