{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4268c662-26d7-403e-8617-f1d69dfe3882",
   "metadata": {},
   "source": [
    "# Data integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589a63c7",
   "metadata": {},
   "source": [
    "A central challenge in most scRNA-seq data analyses is presented by batch effects. Batch effects are transcriptomic manifestations of handling cells in distinct groups or “batches”. For example, a batch effect can arise if two labs have taken lung samples from the same cohort, but these samples are dissociated differently. If lab A optimizes their dissociation protocol to dissociate cells in the sample while minimizing the stress on them, and lab B does not, then it is likely that the cells in the data from the group B will express more stress-linked genes (JUN, JUNB, FOS, etc. see {cite}`Van_den_Brink2017-si`) even if the cells had the same profile in the original tissue. In general, the origins of batch effects are diverse and difficult to pin down. Some batch effect sources might be technical such as differences in sample handling, experimental protocols, or sequencing depths, but also biological effects such as donor variation, tissue, or sampling location are often interpreted as a batch effect (see {cite}`Luecken2021-jo`). Removing these effects is crucial to enable joint analysis that can focus on finding common structure in the data across batches and enable us to perform queries across datasets. Often it is only after removing these effects that one can find rare cellular identities of which there are few per batch, and whose identification was previously obscured by differences between batches. Moreover, queries across datasets allow us to ask questions that could not be answered by single datasets such as _Which cell types express SARS-CoV-2 entry factors and how does this expression differ between individuals?_ {cite}`Muus2021-ti`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc36eb",
   "metadata": {},
   "source": [
    "When removing batch effects from omics data, one must make two central choices: (1) the method and parameterization, and (2) the batch covariate. As batch effects can arise between groupings of cells at different levels (i.e., samples, donors, datasets etc.), the choice of batch covariate indicates which level of variation should be retained and which level removed. The finer the batch resolution, the more effects will be regressed out. However, fine batch variation is also more likely to covary with biologically meaningful signals: Samples typically come from different individuals or different locations in the tissue. These effects may be meaningful to inspect. Thus, choosing the batch covariate will depend on the goal of your integration task: do you want to see differences between individuals, or are you focused on common cell type variation? A principled approach to batch covariate selection was pioneered in a recent effort to build an integrated atlas of the human lung, where the variance attributable to different covariates was used to make this choice {cite}`Sikkema2022-tk`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b3e2e6",
   "metadata": {},
   "source": [
    "Methods that remove batch effects in scRNA-seq are typically composed of (up to) three steps:\n",
    "1. Dimensionality reduction\n",
    "2. Modeling and removing the batch effect\n",
    "3. Projection back into a high-dimensional space\n",
    "\n",
    "While modeling and removing the batch effect (Step 2) is the central part of any method, methods typically project the data to a lower dimensional space (Step 1) to improve the signal-to-noise ratio (see Dimensionality Reduction chapter **CROSS REF TO CHAPTER PREPROCESSING_VISUALIZATION!**) and thereby achieve better performance (see {cite}`Luecken2021-jo`). In the third step, a method may project the data back into the high-dimensional space after removing the fitted batch effect to output a batch-corrected gene expression matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc07c34",
   "metadata": {},
   "source": [
    "Batch-effect removal methods can vary in each of these three steps: they may use various linear or non-linear dimensionality reduction approaches, linear or non-linear batch effect models, and they may output different formats of batch-corrected data. Overall, we can divide methods for batch effect removal into 4 categories. In their order of development, these are: global models, linear embedding models, graph-based methods, and deep learning approaches (Fig I1). \n",
    "\n",
    "**Global models** originate from bulk RNA-seq and model the batch effect as a consistent (additive and/or multiplicative) effect across all cells. A common example is ComBat {cite}`Johnson2007-sl`.\n",
    "\n",
    "**Linear embedding models** were the first single-cell-specific batch removal methods. These approaches often use a variant of singular value decomposition (SVD) to embed the data, then look for local neighborhoods of similar cells across batches in the embedding, which they use to correct the batch effect in a locally adaptive manner. Methods often project the data back into gene expression space using the SVD loadings, but may also only output a corrected embedding. This is the most common group of methods and prominent examples include the pioneering mutual nearest neighbors (MNN) method {cite}`Haghverdi2018-bd`, Seurat integration {cite}`Butler2018-js,Stuart2019-lq`, Scanorama {cite}`Hie2019-er`, FastMNN {cite}`Haghverdi2018-bd`, and Harmony {cite}`Korsunsky2019-ex`.\n",
    "\n",
    "**Graph-based methods** are typically the fastest methods to run. These approaches use some nearest-neighbor graph to represent the data from each batch. Batch effects are corrected by forcing connections between cells from different batches and then allowing for differences in cell type compositions by pruning the forced edges. The most prominent example of these approaches is the batch-balanced k-nearest neighbor (BBKNN) method {cite}`Polanski2019-zy`.\n",
    "\n",
    "**Deep learning (DL) approaches** are the most recent, and most complex methods for batch effect removal that typically require the most data for good performance. These approaches are based on autoencoder networks, and either condition the dimensionality reduction on the batch covariate in a conditional variational autoencoder (CVAE), or fit a locally linear correction in the embedded space. Prominent examples of DL methods are scVI {cite}`Lopez2018-au`, scANVI {cite}`Xu2021-dh`, and scGen {cite}`Lotfollahi2019-cy`.\n",
    "\n",
    "Furthermore, some methods can make use of cell identity labels to provide the method with a reference for what a biological variation should not be removed as batch effect, thereby guiding the fitting and removal of the effect. As batch-effect removal is typically a preprocessing task, such approaches may not applicable to many integration scenarios.\n",
    "\n",
    "More detailed overviews of batch-effect removal methods can be found in {cite}`Argelaguet2021-pb` and {cite}`Luecken2021-jo`. \n",
    "\n",
    "![Overview_fig](figures/integration_overview_figure.jpeg)\n",
    "Fig. I1: Overview of different types of integration methods with examples.\n",
    "\n",
    "\n",
    "**TODO:**\n",
    "- check naming: cell type annotations? Cell identity labels? some combination thereof?\n",
    "- Maybe move the label methods paragraph to benchmarking below?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39f5e50",
   "metadata": {},
   "source": [
    "The removal of batch effects in data has previously been divided into two subtasks: batch correction and data integration {cite}`Luecken2019-og`. These subtasks differ in the complexity of the batch effect that must be removed. Batch correction methods deal with batch effects between samples in the same experiment where cell identity compositions are consistent, and the effect is quasi-linear. In contrast, data integration methods deal with complex, often nested, batch effects between datasets that may be generated with different protocols and where cell identities may not be shared across batches. Given the differences in complexity, it is not surprising that different methods have been benchmarked as being optimal for these two subtasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e7bfe",
   "metadata": {},
   "source": [
    "Several benchmarks have previously evaluated the performance of methods for batch correction and data integration. When removing batch effects, methods may overcorrect and remove meaningful biological variation in addition to the batch effect. Thus, integration performance is formalized by evaluating batch-effect removal and the conservation of biological variation separately.\n",
    "\n",
    "Büttner et al {cite}`Buttner2019-yl` was the first to introduce a dedicated metric to quantify batch removal in kBET. Using kBET, they found that ComBat outperformed other approaches for batch correction while comparing predominantly global models. Building on this, two recent benchmarks {cite}`Tran2020-ia` and {cite}`Chazarra-Gil2021-ri` benchmarked also linear-embedding and deep-learning models on batch correction tasks with few batches or low biological complexity. These studies found that linear-embedding models Seurat {cite}`Butler2018-js,Stuart2019-lq` and Harmony {cite}`Korsunsky2019-ex` performed well for batch correction tasks.\n",
    "\n",
    "Benchmarking data integration tasks poses additional challenges due to both the size of the datasets and the diversity of complex scenarios that may be encountered. Recently, a large study used 14 metrics to benchmark 16 methods across integration method classes on 5 RNA tasks and 2 simulations {cite}`Luecken2021-jo` that present different challenges. While top-performing methods per task differed, approaches that use cell type labels performed better across tasks. Furthermore, deep learning approaches scANVI (with labels), scVI, and scGen (with labels), and linear embedding models Scanorama performed best particularly on complex tasks, while Harmony performed well on less complex tasks. A similar benchmark performed for the specific purpose of integrating retina datasets to build an _ocular mega-atlas_ also found scVI outperformed other methods {cite}`Swamy2021-uy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd36d92",
   "metadata": {},
   "source": [
    "Overall, while integration methods have now been extensively benchmarked the optimal method for all scenarios does not exist. Packages of integration performance metrics and evaluation pipelines like [`scIB`](https://github.com/theislab/scib) and [`batchbench`](https://github.com/cellgeni/batchbench) can be used to evaluate integration performance on ones own data, however cell identity labels are needed for this. Parameter optimization may tune many methods to work for particular tasks, yet in general one can say that Harmony and Seurat consistently perform well for batch correction, and scVI, scGen, scANVI, and Scanorama perform well for data integration. When choosing a method, we would recommend looking into these options first. Additionally, method integration method choice may be guided by requirements on output data formats (i.e., do you need corrected gene expression data or does an integrated embedding suffice?). It would be prudent to test multiple methods before selecting one. Extensive guidelines for data integration method choice can be found in {cite}`Luecken2021-jo`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c0c76",
   "metadata": {},
   "source": [
    "In the notebook below, we demonstrate several methods to integrate diverse batches and evaluate these to guide method choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d88bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python packages\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import bbknn\n",
    "import scib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# R interface\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects import r\n",
    "import rpy2.rinterface_lib.callbacks\n",
    "import anndata2ri\n",
    "\n",
    "pandas2ri.activate()\n",
    "anndata2ri.activate()\n",
    "\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600fbfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# R packages\n",
    "library(Seurat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407aa45e-d853-438a-b85c-ab4f90c353f5",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset we will use to demonstrate data integration contains several samples of bone marrow mononuclear cells. These samples were originally created for the Open Problems in Single-Cell Analysis [NeurIPS Competition 2021](https://openproblems.bio/neurips_2021/) {cite}`Luecken2022-kv,Lance2022-yy`. The [10x Multiome](https://www.10xgenomics.com/products/single-cell-multiome-atac-plus-gene-expression) protocol was used which measures both RNA expression (scRNA-seq) and chromatin accessibility (scATAC-seq) in the same cells. The version of the data we use here has already had some pre-processing.\n",
    "\n",
    "Let's read in the dataset using **scanpy** to get an `AnnData` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc18c7d-09dc-4ea6-9de7-e8b30849747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_raw = sc.read_h5ad(\n",
    "    \"../../datasets/openproblems_bmmc_multiome_genes_filtered.h5ad\"\n",
    ")\n",
    "adata_raw.layers[\"logcounts\"] = adata_raw.X\n",
    "adata_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224db04-1be2-4677-b096-08a7cfd00ede",
   "metadata": {},
   "source": [
    "The full dataset contains 69249 cells and measurements for 129921 features. There are two versions of the expression matrix, `counts` which contains the raw count values and `logcounts` which contains normalised log-counts (these values are also stored in `adata.X`).\n",
    "\n",
    "The `obs` slot contains several variables, some of which were calculated during pre-processing (for quality control) and others that contain metadata about the samples. The ones we are interesed in here are:\n",
    "\n",
    "* `cell_type` - The annotated label for each cell\n",
    "* `batch` - The sequencing batch for each cell\n",
    "\n",
    "For a real analysis it would be important to consider more variables but to keep it simple here we will only look at these.\n",
    "\n",
    "We define variables to hold these names so that it is clear how we are using them in the code. This also helps with reproducibility because if we decided to change one of them for whatever reason we can be sure it has changed in the whole notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc73789-dc59-4ca9-94a1-8f0a552e48b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_key = \"cell_type\"\n",
    "batch_key = \"batch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ed075-ebad-4110-b4af-85d1f23729cd",
   "metadata": {},
   "source": [
    "```{admonition} What to use as the batch label?\n",
    "\n",
    "Deciding what to use as a \"batch\" for data integration is sometimes not easy. The most common approach is to define each sample as a batch (as we have here) which usually produces the strongest batch correction. However, samples are usually confounded with biological factors that you may want to preserve. For example, imagine an experiment that took samples from two locations in a tissue. If samples are considered as batches then data integration methods will attempt to remove differences between them and therefore differences between the locations. In this case it may be more appropriate to use donor as the batch to remove differences between individuals but not between locations. The planned analysis should also be considered. In our example it may be better to have consistent cell type labels for the two locations and then test for differences between then than to have separate clusters for each location which need to be annotated separately and then matched.\n",
    "\n",
    "The confounding between samples and biological factors can be limited through careful experimental design and by using multiplexing techniques which allow biological samples to be combined into a single sequencing sample. However, this is not always possible and requires both extra processing in the lab and as well as extra computational steps.\n",
    "```\n",
    "\n",
    "Let's have a look at the different batches and how many cells we have for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9e8a1-1aca-47fd-b773-52e59e5ae527",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_raw.obs[batch_key].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6031b82-12a4-42a5-a4ae-801df69632a4",
   "metadata": {},
   "source": [
    "There are 13 different different batches in the dataset. During this experiment multiple samples were taken from a set of donors and sequenced at different facilities so the names here are a combination of the sample number (eg. \"s1\") and the donor (eg. \"d2\"). For simplicity, and to reduce computational time, we will select three samples to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b00d15a-05a7-4db9-bac2-545bdb61ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_batches = [\"s1d3\", \"s2d1\", \"s3d7\"]\n",
    "adata = adata_raw[adata_raw.obs[batch_key].isin(keep_batches)].copy()\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec20769-46f4-4b78-9118-5ee6af0ba766",
   "metadata": {},
   "source": [
    "After subsetting to select these batches we are left with 10270 cells.\n",
    "\n",
    "We have two annotations for the features stored in `var`:\n",
    "\n",
    "* `feature_types` - The type of each feature (RNA or ATAC)\n",
    "* `gene_id` - The gene associated with each feature\n",
    "\n",
    "Let's have a look at the feature types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05124435-4297-4a5c-bc0a-a719a2f16e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var[\"feature_types\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0782dc1-8c0c-496c-99fd-99545a93ac52",
   "metadata": {},
   "source": [
    "We can see that there are over 100000 ATAC features but only around 13000 gene expression (\"GEX\") features. Integration of multiple modalities is a complex problem that will be described in other chapters so for now we will subset to only the gene expression features. We also perform a simple filtering to make sure we have no features with zero counts (this is necessary because by selecting a subset of samples we may have removed all the cells which expressed a particular feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf26e2-f47b-4ede-9a15-b80948de2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata[:, adata.var[\"feature_types\"] == \"GEX\"].copy()\n",
    "sc.pp.filter_genes(adata, min_cells=1)\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401373b1-5975-41cc-9978-597010904442",
   "metadata": {},
   "source": [
    "Because of the subsetting we also need to re-normalise the data. Here we just use simple scaling normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc68f61-7351-46c0-bd5a-1a7cbc261d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X = adata.layers[\"counts\"].copy()\n",
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)\n",
    "adata.layers[\"logcounts\"] = adata.X.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1602ad9-4637-4807-a754-55eb1015266e",
   "metadata": {},
   "source": [
    "We will use this dataset to demonstrate integration.\n",
    "\n",
    "Most integration methods require a single object containing all the samples and a batch variable (like we have here). If instead you have separate objects for each of your samples you can join them using the **anndata** `concat()` function. See the [concatenation tutorial](https://anndata.readthedocs.io/en/stable/concatenation.html) for more details. Similar functionality exists in other ecosystems.\n",
    "\n",
    "```{admonition} Integrating UMI and full-length data\n",
    "\n",
    "Integrating samples from UMI and full-length protocols can present additional challenges. This is because full-length protocols are affected by gene-length bias (longer genes will be more highly expressed) while UMI data is not {cite}`Phipson2017-qt`. Because of this, it is generally recommended to transform counts for full-length samples into a unit which corrects for gene-length (such as transcripts per million (TPM) {cite}`Wagner2012-qf`) before attempting integration. This isn't necessary however if all the samples being integrated used a full-length protocol.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0a84f-eb5e-4e0e-b30d-799c0db6d809",
   "metadata": {},
   "source": [
    "## Unintegrated data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0528714-7701-4548-b1a7-c5d27bdde00f",
   "metadata": {},
   "source": [
    "It is always recommended to look at the raw data before performing any integration. This can give some indication of how big any batch effects are and what might be causing them (and therefore which variables to consider as the batch label). For some experiments it might even suggest that integration is not required if samples already overlap. This is not uncommon for mouse or cell line studies from a single lab for example, where most of the variables which contribute to batch effects can be controlled.\n",
    "\n",
    "We will perform highly variable gene (HVG) selection, PCA and UMAP dimensionality reduction as we have seen in previous chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f28cf-4eb0-477b-9384-b2b507926045",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(adata)\n",
    "sc.tl.pca(adata)\n",
    "sc.pp.neighbors(adata)\n",
    "sc.tl.umap(adata)\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e0ff0-1863-4007-b6f1-ca4c1d50ec7e",
   "metadata": {},
   "source": [
    "This adds several new items to our AnnData object. The `var` slot now includes means, dispersions and the selected variable genes. In the `obsp` slot we have distances and connectivities for our KNN graph and in `obsm` are the PCA and UMAP embeddings.\n",
    "\n",
    "Let's plot the UMAP, colouring the points by cell identity and batch labels. If the dataset had not already been labelled (which is often the case) we would only be able to consider the batch labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756f62f-9d4b-4ccd-9a06-6d883d56b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.uns[batch_key + \"_colors\"] = [\n",
    "    \"#1b9e77\",\n",
    "    \"#d95f02\",\n",
    "    \"#7570b3\",\n",
    "]  # Set custom colours for batches\n",
    "sc.pl.umap(adata, color=[label_key, batch_key], wspace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c4f44-ce47-4fa5-ad64-3ba25964e325",
   "metadata": {},
   "source": [
    "Often when looking at these plots you will see clear separation between batches. In this case what we see is more subtle and while cells from the same label are generally near each other there is a shift between batches. If we were to perform a clustering analysis using this raw data we would probably end up with some clusters containing a single batch which would be difficult to interpret at the annotation stage. We are also likely to overlook rare cell types which are not common enough in any single sample to produce their own cluster. While UMAPs can often display batch effects, as always when considering these 2D representations it is important not to over interpret them. For a real analysis you should consider further diagnostic plots that are relevant to your experiment.\n",
    "\n",
    "Now that we have confirmed there are batch effects to correct we can move on to the different integration methods. If the batches perfectly overlaid each other then there would be no need to perform integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c2cf2-e7ca-4eb2-b08f-4ebd0c7bd171",
   "metadata": {},
   "source": [
    "## Batch-aware feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea85702-3887-4e4c-96d7-187d05a9824d",
   "metadata": {},
   "source": [
    "As shown in previous chapters we often select a subset of genes to use for our analysis in order to reduce noise and processing time. We do the same thing when we have multiple samples however, it is important that gene selection is performed in a batch-aware way. This is because genes that are variable across the whole dataset could be capturing batch effects rather than the biological signals we are interested in. It also helps to select genes relevant to rare cell identities, for example if an identity is only present in one sample then markers for it may not be variable across all the samples but should be in that one sample.\n",
    "\n",
    "We can perform batch aware highly variable gene selection by setting the `batch_key` argument in the **scanpy** `highly_variable_genes()` function. **scanpy** will then calculate HVGs for each batch separately and combine the results by selecting those genes that are highly variable in the most batches. We use the **scanpy** function here because it has this batch awareness built in. For other methods we would have to run them on each batch individually and then manually combine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41cad69-ba9e-4738-8df8-dcc8ba2d76d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(\n",
    "    adata, n_top_genes=2000, flavor=\"cell_ranger\", batch_key=batch_key\n",
    ")\n",
    "adata\n",
    "adata.var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03618292-fd1c-447a-9731-387d4132459c",
   "metadata": {},
   "source": [
    "We can see there are now some addtional columns in `var`:\n",
    "\n",
    "* `highly_variable_nbatches` - The number of batches where each gene was found to be highly variable\n",
    "* `highly_variable_intersection` - Whether each genes was highly variable in every batch\n",
    "* `highly_variable` - Whether each genes was selected as highly variable after combining the results from each batch \n",
    "\n",
    "Let's check how many batches each gene was variable in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a134b3-7051-4dda-85f8-eb19bada58e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = adata.var[\"highly_variable_nbatches\"].value_counts()\n",
    "ax = n_batches.plot(kind=\"bar\")\n",
    "n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1e24c-9c9e-4140-8892-2b561e4a1046",
   "metadata": {},
   "source": [
    "The first thing we notice is that most genes are not highly variable. This is typically the case but it can depend on how different the samples we are trying to integrate are. The overlap then decreases as we add more samples, with relatively few genes being highly variable in all three batches. By selecting the top 2000 genes we have selected all HVGs that are present in two or three batches and most of those that are present in one batch.\n",
    "\n",
    "```{admonition} How many genes to use?\n",
    "\n",
    "This is a question which doesn't have a clear answer. The authors of the **scvi-tools** package which we use below recommend between 1000 and 10000 genes but how many depends on the context including the complexity of the dataset and the number of batches. A survey from a previous best-practices paper {cite}`Luecken2019-og` indicated people typically use between 1000 and 6000 HVGs in a standard analysis. While selecting fewer genes can aid in the removal of batch effects {cite}`Luecken2021-jo` (the most highly-variable genes often describe only dominant biological variation), we recommend selecting slightly too many genes rather than selecting too few and risk removing genes which are important for a rare cell type or a pathway of interest. It should however be noted that more genes will also increase the time required to run the integration methods.\n",
    "```\n",
    "\n",
    "We will create an object with just the selected genes to use for integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55795a93-ee0b-41cb-ad12-8e4926b0321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_hvg = adata[:, adata.var[\"highly_variable\"]].copy()\n",
    "adata_hvg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0280220-db89-471e-aab2-6d21d7c605df",
   "metadata": {},
   "source": [
    "## Variational autoencoder (VAE) based integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e83d4c6-526c-4356-bc89-9b1ed3b0ee74",
   "metadata": {},
   "source": [
    "The first integration method we will use is **scVI** (single-cell Variational Inference), a method based on a conditional variational autoencoder {cite}`Lopez2018-au` available in the **scvi-tools** package {cite}`Gayoso2022-ar`. A [variational autoencoder](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73) is a type of artificial neural network which attempts to reduce the dimensionality of a dataset and reconstruct its distribution. The conditional part refers to explicitly considering differences between conditions (in this case batches) during this process in order to remove their effects. In benchmarking studies **scVI** has been shown to perform well across a range of datasets with a good balance of batch correction while conserving biological variability {cite}`Luecken2021-jo`. **scVI** models raw counts directly, so it is important that we provide it with a count matrix rather than a normalized expression matrix.\n",
    "\n",
    "First let's make a copy of our dataset to use for this integration. Normally it is not necessary to do this but as we will demonstrate multiple integration methods making a copy makes it easier to show what has been added by each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4fcc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_scvi = adata_hvg.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eb1637-2e07-4a43-bd05-fd28a6b0a20c",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f2b6f-4158-4311-9aab-f86645fa96b4",
   "metadata": {},
   "source": [
    "The first step in using **scVI** is prepare our AnnData object. This step stores some information required by **scVI** such as which expression matrix to use and what the batch key is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f25a1e0-30e8-4454-aa5f-c8735466862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scvi.model.SCVI.setup_anndata(adata_scvi, layer=\"counts\", batch_key=batch_key)\n",
    "adata_scvi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c9c28f-03d9-4f36-80aa-1abe5c867360",
   "metadata": {},
   "source": [
    "The fields created by **scVI** are prefixed with `_scvi`. These are designed for internal use and should not be manually modified. The general advice from the **scvi-tools** authors is that we should not to make any changes to our object until after the model is trained. On other datasets you may see a warning about the input expression matrix containing unnormalised count data. This usually means you should check that the layer provided to the setup function does actually contain count values but it can also happen if you have values from performing gene length correction on data from a full-length protocol or from another quantification method that does not produce integer counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d596cd4-f1b8-4348-b374-10fc4ad41236",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac9fcb1-a974-4596-9ade-7e17f93dd8fc",
   "metadata": {},
   "source": [
    "We can now construct an **scVI** model object. As well as the **scVI** model we use here, the **scvi-tools** package contains various other models (we will use the **scANVI** model below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951dc72-435d-45d0-a1f5-89df90d713f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scvi = scvi.model.SCVI(adata_scvi)\n",
    "model_scvi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abba8ef-aff1-4ab1-ad7d-ce57d188112c",
   "metadata": {},
   "source": [
    "The model object contains the provided AnnData object as well as the neural network for the model itself. You can see that currently the model is not trained. If we wanted to modify the structure of the network we could provide additional arguments to the model construction function but here we just use the defaults.\n",
    "\n",
    "We can also print a more detailed description of the model that show us where things are stored in the associated AnnData object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa86f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scvi.view_anndata_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0dab4c",
   "metadata": {},
   "source": [
    "Here we can see exactly what information has been assigned by **scVI** and even details like how each different batch is encoded in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf8df9c-27f3-47b0-864e-a2cfcf2481f1",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccdfad4-aec4-469e-bcf7-a94f5ffed7f2",
   "metadata": {},
   "source": [
    "The model will be trained for a given number of _epochs_, a training iteration where every cell is passed through the network. By default **scVI** uses the following heuristic to set the number of epochs. For datasets with fewer than 20000 cells 400 epochs will be used and this decreases as the number of cells goes above 20000. The reasoning behind this is that as the network sees more cells during each epoch it can learn the same amount of information as it would from more epochs with fewer cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462f886-7172-4f1a-8af8-614ac2915049",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs_scvi = np.min([round((20000 / adata.n_obs) * 400), 400])\n",
    "max_epochs_scvi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8e6c97-709a-4a3a-8c5d-2cf8a0d3538a",
   "metadata": {},
   "source": [
    "We now train the model for the selected number of epochs (this will take ~20-40 minutes depending on the computer you are using)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441672ad-d277-4bef-8112-51d0bb22c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scvi.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b4d75f-8661-441f-b721-f7a7895a88fa",
   "metadata": {},
   "source": [
    "```{admonition} Early stopping\n",
    "\n",
    "An alternative to manually setting the number of epochs is to set `early_stopping=True` in the training function. This will let **scVI** decide to stop training early depending on the convergence of the model. The exact conditions for stopping can be controlled by other parameters.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7623937-2b97-4a37-96cc-4bf390a57468",
   "metadata": {},
   "source": [
    "### Extracting the embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac668386-f7cf-4a61-a9ab-c675a9a25d0b",
   "metadata": {},
   "source": [
    "The main result we want to extract from the trained model is the latent representation for each cell. This is an embedding where the batch effects have been removed that can be used in a similar way to how we use PCA dimensions when analysing a single dataset. We store this in `obsm` with the key `X_scvi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f0055-44e6-45f4-b360-8cb6995594ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_scvi.obsm[\"X_scVI\"] = model_scvi.get_latent_representation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80242f0-18b2-4d60-9082-ef6a24dea58e",
   "metadata": {},
   "source": [
    "### Calculate a batch-corrected UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e9166-6cf1-4ab8-bae3-87649af62923",
   "metadata": {},
   "source": [
    "We will now visualise the data as we did before integration. We calculate a new UMAP embedding but instead of finding nearest neighbours in PCA space we start with the corrected representation from **scVI**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd09d398-bf54-46a1-bade-711cb05662fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata_scvi, use_rep=\"X_scVI\")\n",
    "sc.tl.umap(adata_scvi)\n",
    "adata_scvi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d83a5b-25c8-4919-b766-c863c9daf148",
   "metadata": {},
   "source": [
    "Once we have the new UMAP representation we can plot it coloured by batch and identity labels as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a68931-1346-4d5f-a017-926a8c29e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_scvi, color=[label_key, batch_key], wspace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29df77f-dead-45a8-8030-e3bfcc079f5e",
   "metadata": {},
   "source": [
    "This looks better! Before, the various batches were shifted apart from each other. Now, the batches are overlap more and we have a single blob for each cell identity label.\n",
    "\n",
    "In many cases we would not already have identity labels so from this stage we would continue with clustering, annotation and further analysis as described in other chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88da50d0-d51f-4bb2-bf69-3a9fa6ec5165",
   "metadata": {},
   "source": [
    "## VAE integration using cell labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698a47a-2b78-4594-a0d1-c3f02fae9003",
   "metadata": {},
   "source": [
    "When performing integration with **scVI** we pretended that we didn't already have any cell labels (although we showed them in plots). While this scenario is common there are some cases where we do know something about cell identity in advance. Most often this is when we want to combine one or more publicly available datasets with data from a new study. When we have labels for at least some of the cells we can use **scANVI** (single-cell ANnotation using Variational Inference) {cite}`Xu2021-dh`. This is an extension of the **scVI** model that knows about cell labels as well as batches. Because it has this extra information it can try to keep the differences between cell labels while removing batch effects. Benchmarking suggests that **scANVI** tends to better preserve biological signals compared to **scVI** but sometimes it is not as effective at removing batch effects {cite}`Luecken2021-jo`. While we have labels for all cells here it is also possible to use **scANVI** in a semi-supervised manner where labels are only provided for some cells.\n",
    "\n",
    "```{admonition} Label harmonization\n",
    "\n",
    "If you are using **scANVI** to integrate multiple datasets for which you already have labels it is important to first perform _label harmonization_. This refers to a process of checking that labels are consistent across the datasets that are being integrated. How best to do this is an open question but often requires input from subject-matter experts.\n",
    "```\n",
    "\n",
    "We start by creating a **scANVI** model object. Note that because **scANVI** refines an **scVI** model we provide that rather than an AnnData object. If we had not already trained an **scVI** model we would need to do that first. We also provide a key for the column of `adata.obs` which contains our cell labels as well as the label which corresponds to unlabelled cells. In this case all of our cells are labelled so we just provide a dummy value but in most cases it is important to check that this is set correctly so that **scANVI** knows which label to ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67bd4c-88ff-4ffc-8ffc-da7e85ac0e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally we would need to run scVI first but we have already done that here\n",
    "# model_scvi = scvi.model.SCVI(adata_scvi) etc.\n",
    "model_scanvi = scvi.model.SCANVI.from_scvi_model(\n",
    "    model_scvi, labels_key=label_key, unlabeled_category=\"unlabelled\"\n",
    ")\n",
    "print(model_scanvi)\n",
    "model_scanvi.view_anndata_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a86948b-36ff-48e0-bd9a-74738d55fea8",
   "metadata": {},
   "source": [
    "This model object is very similar to what we saw before for **scVI**. As mentioned previously we could modify the structure of the model network but here we just use the default.\n",
    "\n",
    "Again we have a heuristic for selecting the number of training epochs. Note that this is much fewer than before as we are just refining the **scVI** model, rather than training a whole network from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af3bbc-6a34-4797-be80-dd192fb647ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs_scanvi = int(np.min([10, np.max([2, round(max_epochs_scvi / 3.0)])]))\n",
    "model_scanvi.train(max_epochs=max_epochs_scanvi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1908c8bb-89dc-4cf0-9132-272c6a454afd",
   "metadata": {},
   "source": [
    "We can extract the new latent representation from the model and create a new UMAP embedding as we did for **scVI**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a7a7c1-1f71-4455-9238-2b7ff1ba62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_scanvi = adata_scvi.copy()\n",
    "adata_scanvi.obsm[\"X_scANVI\"] = model_scanvi.get_latent_representation()\n",
    "sc.pp.neighbors(adata_scanvi, use_rep=\"X_scANVI\")\n",
    "sc.tl.umap(adata_scanvi)\n",
    "sc.pl.umap(adata_scanvi, color=[label_key, batch_key], wspace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b56b0-6111-4be1-8855-0a2db59e0cf3",
   "metadata": {},
   "source": [
    "By looking at the UMAP representation it is difficult to tell the difference between **scANVI** and **scVI** but as we will see below there are differences in metric scores when the quality of the integrations is quantified. This is a reminder that we shouldn't over interpret these two-dimensional representations, especially when it comes to comparing methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d4119-fb74-4bbe-8e90-95993fa797f4",
   "metadata": {},
   "source": [
    "## Graph-based integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d0642",
   "metadata": {},
   "source": [
    "The next method we will look at is **BBKNN** or \"Batch Balanced KNN\" {cite}`Polanski2019-zy`. This is a very different approach to **scVI**, rather than using a neural network to embed cells in a batch corrected space it instead modifies how the _k_-nearest neighbour (KNN) graph used for clustering and embedding is constructed. As we have seen in previous chapters the normal KNN procedure connects cells to the most similar cells across the whole dataset. The change that **BBKNN** makes is to enforce that cells are connected to cells from other batches. While this is a simple modification it can be quite effective, particularly when there are very strong batch effects. However, as the output is an integrated graph it can have limited downstream uses as few packages will accept this as an input.\n",
    "\n",
    "An important parameter for **BBKNN** is the number of neighbours per batch. A suggested heuristic for this is to use 25 if there are more than 100000 cells or the default of 3 if there are fewer than 100000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c1b3d9-d012-40bb-b023-cbfec156b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_within_batch = 25 if adata_hvg.n_obs > 100000 else 3\n",
    "neighbors_within_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e21473",
   "metadata": {},
   "source": [
    "Before using **BBKNN** we first perform a PCA as we would before building a normal KNN graph. Unlike **scVI** which models raw counts here we start with the log-normalised expression matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bd01e4-397f-4264-a450-6d1734c019a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_bbknn = adata_hvg.copy()\n",
    "adata_bbknn.X = adata_bbknn.layers[\"logcounts\"].copy()\n",
    "sc.pp.pca(adata_bbknn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b73890",
   "metadata": {},
   "source": [
    "We can now run **BBKNN**, replacing the call to the **scanpy** `neighbors()` function in a standard workflow. An important difference is to make sure the `batch_key` argument is set which specifies a column in `adata_hvg.obs` which contains batch labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4169c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbknn.bbknn(\n",
    "    adata_bbknn, batch_key=batch_key, neighbors_within_batch=neighbors_within_batch\n",
    ")\n",
    "adata_bbknn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cf96aa",
   "metadata": {},
   "source": [
    "Unlike the default **scanpy** function **BBKNN** does not allow specifying a key for storing results so they are always stored under the default \"neighbors\" key.\n",
    "\n",
    "We can use this new integrated graph just like we would a normal KNN graph to construct a UMAP embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a048ce-5b85-4778-a37e-2f5f18eb2041",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.umap(adata_bbknn)\n",
    "sc.pl.umap(adata_bbknn, color=[label_key, batch_key], wspace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4a891",
   "metadata": {},
   "source": [
    "This integration is also improved compared to the unintegrated data with cell identities grouped together but we sill see some shifts between batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd2b7d",
   "metadata": {},
   "source": [
    "## Linear embedding integration using Mutual Nearest Neighbors (MNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16158b83",
   "metadata": {},
   "source": [
    "Some downstream applications cannot accept an integrated embedding or neighborhood graph and require a corrected expression matrix. One approach that can produce this output is the integration method in **Seurat** {cite}`Satija2015-or,Butler2018-js,Stuart2019-lq`. The **Seurat** integration method belongs to a class of _linear embedding models_ that make use of the idea of _mutual nearest neighbors_ (which **Seurat** calls _anchors_) to correct batch effects {cite}`Haghverdi2018-bd`. Mutual nearest neighbors are pairs of cells from two different datasets which are in the neighborhood of each other when the datasets are placed in the same (latent) space. After finding these cells they can be used to align the two datasets and correct the differences between them. **Seurat** has also been found to one of the top mixing methods in some evaluations {cite}`Tran2020-ia`.\n",
    "\n",
    "As **Seurat** is an R package we must transfer our data from Python to R. Here we prepare the object to convert so that it can be handled by **rpy2** and **anndata2ri**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c8b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_seurat = adata_hvg.copy()\n",
    "# Convert categorical columns to strings\n",
    "adata_seurat.obs[batch_key] = adata_seurat.obs[batch_key].astype(str)\n",
    "adata_seurat.obs[label_key] = adata_seurat.obs[label_key].astype(str)\n",
    "# Delete uns as this can contain arbitrary objects which are difficult to convert\n",
    "del adata_seurat.uns\n",
    "adata_seurat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a51a52",
   "metadata": {},
   "source": [
    "The prepared object is now available in R as a SingleCellExperiment object thanks to **anndata2ri**. Note that this is transposed compared to an AnnData object so our observations (cells) are now the columns and our variables (genes) are now the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i adata_seurat\n",
    "adata_seurat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1a294c",
   "metadata": {},
   "source": [
    "**Seurat** uses it's own object to store data. Helpfully the authors provide a function to convert from SingleCellExperiment. We just provide the object and tell **Seurat** which assays (layers in our AnnData object) contain raw counts and normalised expression (which **Seurat** stores in a slot called \"data\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc42b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i adata_seurat\n",
    "seurat <- as.Seurat(adata_seurat, counts = \"counts\", data = \"logcounts\")\n",
    "seurat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a3b54",
   "metadata": {},
   "source": [
    "Unlike some of the other methods we have seen which take a single object and a batch key, the **Seurat** integration functions require a list of objects. We create this using the `SplitObject()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i batch_key\n",
    "batch_list <- SplitObject(seurat, split.by = batch_key)\n",
    "batch_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f6d656",
   "metadata": {},
   "source": [
    "We can now use this list to find anchors for each pairs of datasets. Usually you would identify batch-aware highly variable genes first (using the `FindVariableFeatures()` and `SelectIntegrationFeatures()` functions) but as we have done that already we tell **Seurat** to use all the features in the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0af225",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "anchors <- FindIntegrationAnchors(batch_list, anchor.features = rownames(seurat))\n",
    "anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bfe6b7",
   "metadata": {},
   "source": [
    "**Seurat** can then use the anchors to compute a transformation which maps one dataset onto another. This is done in a pairwise way until all the datasets are merged. By default **Seurat** will determine a merge order so that more similar datasets are merged together first but it is also possible to define this order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b0f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "integrated <- IntegrateData(anchors)\n",
    "integrated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee47aa2d",
   "metadata": {},
   "source": [
    "The result is another Seurat object, but notice now that the active assay is called \"integrated\". This contains the corrected expression matrix which is the final output of the integration.\n",
    "\n",
    "Here we extract that matrix and prepare it for transfer back to Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8ca95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o integrated_expr\n",
    "# Extract the integrated expression matrix\n",
    "integrated_expr <- GetAssayData(integrated)\n",
    "# Make sure the rows and columns are in the same order as the original object\n",
    "integrated_expr <- integrated_expr[rownames(seurat), colnames(seurat)]\n",
    "# Transpose the matrix to AnnData format\n",
    "integrated_expr <- t(integrated_expr)\n",
    "print(integrated_expr[1:10, 1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14610941",
   "metadata": {},
   "source": [
    "We will now store the corrected expression matrix as a layer in our AnnData object. We also set `adata.X` to use this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_seurat.X = integrated_expr\n",
    "adata_seurat.layers[\"seurat\"] = integrated_expr\n",
    "print(adata_seurat)\n",
    "adata.X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb8085",
   "metadata": {},
   "source": [
    "Now that we have the results of our integration we can calculate a UMAP and plot it as we have for the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the batch colours because we deleted them earlier\n",
    "adata_seurat.uns[batch_key + \"_colors\"] = [\n",
    "    \"#1b9e77\",\n",
    "    \"#d95f02\",\n",
    "    \"#7570b3\",\n",
    "]\n",
    "sc.tl.pca(adata_seurat)\n",
    "sc.pp.neighbors(adata_seurat)\n",
    "sc.tl.umap(adata_seurat)\n",
    "sc.pl.umap(adata_seurat, color=[label_key, batch_key], wspace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2034b44a",
   "metadata": {},
   "source": [
    "As we have previously seen, the batches are mixed while the labels are separated. It is tempting to select an integration based on the UMAPs but this does not fully represent the quality of an integration. In the next section we present some approaches to more rigorously evaluate integration methods.\n",
    "\n",
    "```{admonition} A note on scalability\n",
    "\n",
    "As you ran the different integration methods you may have noticed that **scVI** took the most time. While this is true for small datasets like the example shown here, [benchmarks have shown](https://www.nature.com/articles/s41592-021-01336-8/figures/13) that **scVI** scales well for larger datasets. This is largely because the number of training epochs is adjusted for larger dataset sizes. MNN methods typically don't scale as well, partly because they perform several pairwise integrations, so if you have 20 batches you are performing 20 integrations while other methods can consider all batches at once.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca76892-58b2-48ef-bf59-6df49f5272da",
   "metadata": {},
   "source": [
    "## Benchmarking your own integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd082b86-3185-4e32-9589-061ad7acf772",
   "metadata": {},
   "source": [
    "The methods demonstrated here are selected based on results from benchmarking experiments including the [single-cell integration benchmarking project](https://theislab.github.io/scib-reproducibility/) {cite}`Luecken2021-jo`. This project also produced a software package called [**scib**](https://www.github.com/theislab/scib) that can be used to run a range of integration methods as well as the metrics that were used for evaluation. In this section we show how to use this package to evaluate the quality of an integration.\n",
    "\n",
    "```{admonition} What is the ground truth?\n",
    "\n",
    "Some of these metrics, particularly those that evaluate conservation of biological variation, require a known ground truth to compare to. Usually this is a cell identity label but can sometimes be other information such as known trajectories. Because of this requirement it is difficult to evaluate integration for a completely new dataset where it is unclear what biological signal should be preserved.\n",
    "```\n",
    "\n",
    "The **scib** metrics can be run individually but there are also wrappers for running multiple metrics at once. Here we run a subset of the metrics which are quick to compute using the `metrics_fast()` function. This function takes a few arguments: the original unintegrated dataset, the integrated dataset, a batch key and a label key. Depending on the output of the integration method we might also need to supply additional arguments, for example here we specify the embedding to use for **scVI** and **scANVI** with the `embed` argument. You can also control how some metrics are run with additional arguments.\n",
    "\n",
    "Let's run the metrics for each of the integrations we have performed above, as well as the unintegrated data (after highly variable gene selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c847fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_scvi = scib.metrics.metrics_fast(\n",
    "    adata, adata_scvi, batch_key, label_key, embed=\"X_scVI\"\n",
    ")\n",
    "metrics_scanvi = scib.metrics.metrics_fast(\n",
    "    adata, adata_scanvi, batch_key, label_key, embed=\"X_scANVI\"\n",
    ")\n",
    "metrics_bbknn = scib.metrics.metrics_fast(adata, adata_bbknn, batch_key, label_key)\n",
    "metrics_seurat = scib.metrics.metrics_fast(adata, adata_seurat, batch_key, label_key)\n",
    "metrics_hvg = scib.metrics.metrics_fast(adata, adata_hvg, batch_key, label_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d3d2a3",
   "metadata": {},
   "source": [
    "Here is an example of what one of the metrics results looks like for a single integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_hvg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bcffe3",
   "metadata": {},
   "source": [
    "Each row is a different metric and the values show the score for that metric. Scores are between 0 and 1, where 1 is a good performance and 0 is a poor performance (**scib** can also return unscaled scores for some metrics if required). Because we have only run the fast metrics here, some of the metrics have `NaN` scores. Also note that some metrics cannot be used with some output formats which can also be a reason for `NaN` values being returned.\n",
    "\n",
    "To compare the methods it is useful to have all the metrics results in one table. This code combines them and tidies them into a more convenient format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc53f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate metrics results\n",
    "metrics = pd.concat(\n",
    "    [metrics_scvi, metrics_scanvi, metrics_bbknn, metrics_seurat, metrics_hvg],\n",
    "    axis=\"columns\",\n",
    ")\n",
    "# Set methods as column names\n",
    "metrics = metrics.set_axis(\n",
    "    [\"scVI\", \"scANVI\", \"BBKNN\", \"Seurat\", \"Unintegrated\"], axis=\"columns\"\n",
    ")\n",
    "# Select only the fast metrics\n",
    "metrics = metrics.loc[\n",
    "    [\n",
    "        \"ASW_label\",\n",
    "        \"ASW_label/batch\",\n",
    "        \"PCR_batch\",\n",
    "        \"isolated_label_silhouette\",\n",
    "        \"graph_conn\",\n",
    "        \"hvg_overlap\",\n",
    "    ],\n",
    "    :,\n",
    "]\n",
    "# Transpose so that metrics are columns and methods are rows\n",
    "metrics = metrics.T\n",
    "# Remove the HVG overlap metric because it's not relevant to embedding outputs\n",
    "metrics = metrics.drop(columns=[\"hvg_overlap\"])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fdad87",
   "metadata": {},
   "source": [
    "We now have all the scores in one table with metrics as columns and methods as rows. Styling the table with a gradient can make it easier to see the differences between scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca8fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfbaede",
   "metadata": {},
   "source": [
    "For some metrics the scores tend to be in a relatively small range. To emphasise the differences between methods and place each metric on the same scale, we scale them so that the worst performer gets a score of 0, the best performer gets a score of 1 and the others are somewhere in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c01aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_scaled = (metrics - metrics.min()) / (metrics.max() - metrics.min())\n",
    "metrics_scaled.style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124b044",
   "metadata": {},
   "source": [
    "The values now better represent the differences between methods (and better match the colour scale). However, it is important to note that the scaled scores can only be used to compare the relative performance of this specific set of integrations. If we wanted to add another method we would need to perform the scaling again. We also can't say that an integration is definitively \"good\", only that it is better than the other methods we have tried. This scaling emphasises differences between methods. For example, if we had metric scores of 0.92, 0.94 and 0.96 these would be scaled to 0, 0.5 and 1.0. This makes the first method appear to score much worse, even though it is only slightly lower than the other two and still got a very high score. This effect is bigger when you are comparing few methods and when they get similar raw scores. Whether you look at raw or scaled scores depends on whether you want to focus on absolute performance or difference in performance between methods.\n",
    "\n",
    "The evaluation metrics can be grouped into two categories, those that measure the removal of batch effects and those that measure the conservation of biological variation. We can calculate summary scores for each of these categories by taking the mean of the scaled values for each group. This kind of summary score wouldn't make sense with raw values as some metrics consistently produce higher scores than others (and therefore have a greater effect on the mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9147a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_scaled[\"Batch\"] = metrics_scaled[\n",
    "    [\"ASW_label/batch\", \"PCR_batch\", \"graph_conn\"]\n",
    "].mean(axis=1)\n",
    "metrics_scaled[\"Bio\"] = metrics_scaled[[\"ASW_label\", \"isolated_label_silhouette\"]].mean(\n",
    "    axis=1\n",
    ")\n",
    "metrics_scaled.style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f174af3",
   "metadata": {},
   "source": [
    "Plotting the two summary scores against each other gives an indication of the priorities of each method. Some will be biased towards batch correction while others will favour retaining biological variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adea52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "metrics_scaled.plot.scatter(\n",
    "    x=\"Batch\",\n",
    "    y=\"Bio\",\n",
    "    c=range(len(metrics_scaled)),\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "for k, v in metrics_scaled[[\"Batch\", \"Bio\"]].iterrows():\n",
    "    ax.annotate(\n",
    "        k,\n",
    "        v,\n",
    "        xytext=(6, -3),\n",
    "        textcoords=\"offset points\",\n",
    "        family=\"sans-serif\",\n",
    "        fontsize=12,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d5a24",
   "metadata": {},
   "source": [
    "In our small example scenario **BBKNN** is clearly the worst performer, getting the lowest scores for both batch removal an biological conservation. The other three methods have similar batch correction scores with **scANVI** scoring highest for biological conservation followed by **scVI** and **Seurat**.\n",
    "\n",
    "To get an overall score for each method we can combine the two summary scores. The **scIB** paper suggests a weighting of 40% batch correction and 60% biological conservation but you may prefer to weight things differently depending on the priorities for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a96df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_scaled[\"Overall\"] = 0.4 * metrics_scaled[\"Batch\"] + 0.6 * metrics_scaled[\"Bio\"]\n",
    "metrics_scaled.style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3efe89",
   "metadata": {},
   "source": [
    "Let's make a quick bar chart to visualise the overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c140fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_scaled.plot.bar(y=\"Overall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cae93f",
   "metadata": {},
   "source": [
    "As we have already seen **scVI** and **scANVI** are the best performers with **scANVI** scoring slightly higher. It is important to note that this is just an example of how to run these metrics for this specific dataset, not a proper evaluation of these methods. For that you should refer to existing benchmarking publications. In particular, we have only run a small selection of high-performing methods and a subset of metrics here. Also remember that scores are relative to the methods used so even if the methods perform almost equally well and small differences will be exaggerated.\n",
    "\n",
    "Existing benchmarks have suggested methods which generally perform well, but performance can also be quite variable across scenarios. For some analyses it may be worthwhile performing your own evaluation of integration. The **scib** package makes this process easier, but it can still be a significant undertaking, relying on a good knowledge of the ground truth and interpretation of the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e36f5",
   "metadata": {},
   "source": [
    "## Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b8b501",
   "metadata": {},
   "source": [
    "1. Visualize your data before attempting to correct for batch effects to assess the extent of the issue. Batch effect correction is not always required and it might mask the biological variation of interest.\n",
    "2. If cell labels are available and biological variation is the most important, the usage of methods that can use these labels (such as scANVI) is advised.\n",
    "3. Consider running several integration methods on your dataset and evaluating them with the **scIB** metrics to use the integration that is most robust for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7834cf1",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f30ba2f",
   "metadata": {},
   "source": [
    "1. What are sources of batch effects?\n",
    "2. What is the difference between technical and biological variation?\n",
    "3. How does one evaluate whether the integration worked well or not? What are useful metrics for this purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ffb412-d8e2-471e-bdf9-77ef33891fe9",
   "metadata": {},
   "source": [
    "## Session information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60abf3b",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f706f-d575-4f17-821e-b257d1bc6a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import session_info\n",
    "\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39a27c",
   "metadata": {},
   "source": [
    "### R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd0c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "sessioninfo::session_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dd887f",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a5667b",
   "metadata": {},
   "source": [
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    ":labelprefix: int\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79aba2",
   "metadata": {},
   "source": [
    "## Contributors\n",
    "\n",
    "We gratefully acknowledge the contributions of:\n",
    "\n",
    "### Authors\n",
    "\n",
    "* Luke Zappia\n",
    "* Malte Lücken\n",
    "\n",
    "### Reviewers\n",
    "\n",
    "* Lukas Heumos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3.8.12 ('bp-integration')"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3.9.13"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3 (ipykernel)"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3.8.9"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  },
  "vscode": {
   "interpreter": {
    "hash": "a83cbf0d97398c686b64d26c4d5e629672b46e2359271440e52b1d6132c22eb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
